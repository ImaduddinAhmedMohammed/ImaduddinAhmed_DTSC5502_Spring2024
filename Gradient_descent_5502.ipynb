{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaduddinAhmedMohammed/ImaduddinAhmed_DTSC5502_Spring2024/blob/main/Gradient_descent_5502.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBjyc5Empc5e",
        "outputId": "11debf10-3235-44e9-89eb-a969af836bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  0  1  4  3  2  5  6  9 13 15 16]\n",
            "\n",
            "(12,)\n",
            "\n",
            "X is:\n",
            "\n",
            "[[ 1  1  1]\n",
            " [ 1  2  1]\n",
            " [ 1  2  2]\n",
            " [ 1  3  2]\n",
            " [ 1  5  4]\n",
            " [ 1  5  6]\n",
            " [ 1  6  5]\n",
            " [ 1  7  4]\n",
            " [ 1 10  8]\n",
            " [ 1 11  7]\n",
            " [ 1 11  9]\n",
            " [ 1 12 10]]\n",
            "\n",
            "(12, 3)\n",
            "\n",
            "X Transpose:\n",
            "\n",
            "[[ 1  1  1  1  1  1  1  1  1  1  1  1]\n",
            " [ 1  2  2  3  5  5  6  7 10 11 11 12]\n",
            " [ 1  1  2  2  4  6  5  4  8  7  9 10]]\n",
            "(3, 12)\n",
            "\n",
            "shape of beta is:  (3, 1)\n",
            "Iteration: 0 \tBeta values: [0.00125    0.0117     0.00903333]\n",
            "Best loss: 796.8220357944444\n",
            "Iteration: 1 \tBeta values: [0.00247624 0.02319901 0.01790875]\n",
            "Best loss: 771.5422005482003\n",
            "Iteration: 2 \tBeta values: [0.00367914 0.0345005  0.02662899]\n",
            "Best loss: 747.1296763701905\n",
            "Iteration: 3 \tBeta values: [0.00485909 0.04560789 0.03519673]\n",
            "Best loss: 723.5547028377974\n",
            "Iteration: 4 \tBeta values: [0.0060165  0.05652455 0.04361462]\n",
            "Best loss: 700.7885407114618\n",
            "Iteration: 5 \tBeta values: [0.00715175 0.06725377 0.05188524]\n",
            "Best loss: 678.803436894359\n",
            "Iteration: 6 \tBeta values: [0.00826523 0.07779879 0.06001115]\n",
            "Best loss: 657.5725905944303\n",
            "Iteration: 7 \tBeta values: [0.00935732 0.08816281 0.06799484]\n",
            "Best loss: 637.0701206475107\n",
            "Iteration: 8 \tBeta values: [0.01042839 0.09834896 0.07583879]\n",
            "Best loss: 617.2710339617158\n",
            "Iteration: 9 \tBeta values: [0.01147879 0.10836031 0.08354542]\n",
            "Best loss: 598.1511950446079\n",
            "Iteration: 29991 \tBeta values: [-1.77563812  1.49036395 -0.23632086]\n",
            "Best loss: 34.86201211692461\n",
            "Iteration: 29992 \tBeta values: [-1.77566357  1.49036865 -0.23632301]\n",
            "Best loss: 34.861931224765506\n",
            "Iteration: 29993 \tBeta values: [-1.77568901  1.49037334 -0.23632516]\n",
            "Best loss: 34.86185034137409\n",
            "Iteration: 29994 \tBeta values: [-1.77571445  1.49037804 -0.23632731]\n",
            "Best loss: 34.86176946674937\n",
            "Iteration: 29995 \tBeta values: [-1.77573989  1.49038273 -0.23632945]\n",
            "Best loss: 34.86168860089032\n",
            "Iteration: 29996 \tBeta values: [-1.77576533  1.49038743 -0.2363316 ]\n",
            "Best loss: 34.86160774379602\n",
            "Iteration: 29997 \tBeta values: [-1.77579077  1.49039212 -0.23633374]\n",
            "Best loss: 34.861526895465424\n",
            "Iteration: 29998 \tBeta values: [-1.77581621  1.49039682 -0.23633589]\n",
            "Best loss: 34.8614460558976\n",
            "Iteration: 29999 \tBeta values: [-1.77584165  1.49040151 -0.23633804]\n",
            "Best loss: 34.861365225091504\n",
            "Iteration: 30000 \tBeta values: [-1.77586708  1.4904062  -0.23634018]\n",
            "Best loss: 34.86128440304624\n",
            "shape of gradient is: (3,)\n",
            "shape of y pred is: (12,)\n",
            "shape of y pred - Y is: (12,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "Y= np.array([1,0,1,4,3,2,5,6,9,13,15,16])\n",
        "print(Y)\n",
        "print(\"\")\n",
        "print(Y.shape)\n",
        "print(\"\")\n",
        "X = np.array([ [1,1,1],\n",
        "               [1,2,1],\n",
        "               [1,2,2],\n",
        "               [1,3,2],\n",
        "               [1,5,4],\n",
        "               [1,5,6],\n",
        "               [1,6,5],\n",
        "               [1,7,4],\n",
        "               [1,10,8],\n",
        "               [1,11,7],\n",
        "               [1,11,9],\n",
        "               [1,12,10] ])\n",
        "print(\"X is:\")\n",
        "print(\"\")\n",
        "print(X)\n",
        "print(\"\")\n",
        "print(X.shape)\n",
        "print(\"\")\n",
        "XT=np.transpose(X)\n",
        "print(\"X Transpose:\")\n",
        "print(\"\")\n",
        "print(XT)\n",
        "print(XT.shape)\n",
        "print(\"\")\n",
        "\n",
        "def linear_regression(beta,X,Y):      #defining a function for linear regression to calculate the loss\n",
        "  Y_pred= X @ beta\n",
        "  loss = np.sum((Y_pred - Y) ** 2)\n",
        "  return loss\n",
        "\n",
        "def gradient_descent(X,Y,learning_rate,num_iterations): #defining a function for gradient descent\n",
        "  n,p=X.shape\n",
        "  beta=np.zeros(p)                                      #intitalizing the values in beta matrix to 0\n",
        "  Beta=beta.reshape(-1, 1)\n",
        "  print(\"shape of beta is: \",Beta.shape)\n",
        "  best_loss=np.inf                                      #initializing the best loss to infintite and then update it later\n",
        "  best_beta=np.zeros(p)                                 #initializing the values in best beta matrix to 0\n",
        "  for i in range(num_iterations+1):                     #iterating a loop to execute and calculate the best beta and loss at every set of beta values\n",
        "    Y_pred=np.dot(X,beta)\n",
        "    gradient = 2 * X.T @ (Y_pred - Y) / n\n",
        "    beta = beta-(learning_rate*gradient)\n",
        "    current_loss=linear_regression(beta,X,Y)            # calling the linear_regression function to find the loss\n",
        "    if current_loss<best_loss:                          # comparing the best loss and current loss and updating the loss according to the condition\n",
        "      best_loss=current_loss\n",
        "      best_beta=beta\n",
        "    if i<10 or i>num_iterations-10:                     # setting condition to print only the first 10 values and the last 10 values\n",
        "      print(\"Iteration:\",i,\"\\tBeta values:\",beta)\n",
        "      print(\"Best loss:\",current_loss)\n",
        "  print(\"shape of gradient is:\",gradient.shape)\n",
        "  print(\"shape of y pred is:\",Y_pred.shape)\n",
        "  print(\"shape of y pred - Y is:\",(Y_pred-Y).shape)\n",
        "  return{'beta': best_beta, 'loss': best_loss}\n",
        "\n",
        "learning_rate= 0.0001                                   #setting the learning rate\n",
        "num_iterations=30000                                    #setting the number of iterations\n",
        "result=gradient_descent(X,Y,learning_rate,num_iterations)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}